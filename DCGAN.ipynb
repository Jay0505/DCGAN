{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.layers import batch_normalization\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class helper_functions:\n",
    "    \n",
    "    def get_weights(self, shape, name):\n",
    "        with tf.variable_scope('weights', reuse = tf.AUTO_REUSE):\n",
    "            wt_init = tf.random_normal_initializer()\n",
    "            return tf.get_variable(name = name, shape = shape, initializer = wt_init)\n",
    "    \n",
    "    \n",
    "    def get_bias(self, shape, name):\n",
    "        with tf.variable_scope('biases', reuse = tf.AUTO_REUSE):\n",
    "            init = tf.constant_initializer(0)\n",
    "            return tf.get_variable(name = name, shape = shape, initializer = init)\n",
    "\n",
    "\n",
    "    def conv_layer(self, data, weights, bias, name, strides, batch_normalize = False, discriminator = True):\n",
    "\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            conv_res         = tf.nn.conv2d(data, filter = weights, strides = strides, padding = 'SAME')\n",
    "            \n",
    "            # we add bias to the conv_res only if it is discriminator\n",
    "            if discriminator:    \n",
    "                conv_res     = tf.nn.bias_add(conv_res, bias)\n",
    "\n",
    "            if batch_normalize:\n",
    "                conv_bn      = batch_normalization(conv_res, momentum = 0.5)\n",
    "                conv_bn_relu = tf.nn.leaky_relu(conv_bn)\n",
    "                return conv_bn_relu\n",
    "            \n",
    "            if not batch_normalize:\n",
    "                conv_relu = tf.nn.leaky_relu(conv_res)\n",
    "                return conv_relu\n",
    "            \n",
    "      \n",
    "    def conv_block(self, data, weights, bias, block_name, strides, first_or_last_block = False, disc = True):\n",
    "        \n",
    "        # first block -> discriminator || last_block -> generator\n",
    "        # we don't batch_normalize data for first conv block in discrminator and last block in generator\n",
    "        batch_normalize = False if first_or_last_block else True\n",
    "        conv_result     = self.conv_layer(data, weights, bias, block_name, strides, batch_normalize, disc)\n",
    "        return conv_result\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_flatten_data_shape(self, data_shape):\n",
    "        total_neurons = 1\n",
    "        for val in data_shape:\n",
    "            if val is not None:\n",
    "                total_neurons = total_neurons * val\n",
    "        return total_neurons\n",
    "    \n",
    "    \n",
    "    def fully_connected_layer(self, data, shape_flatten, weights, bias, block_name):\n",
    "#         total_neurons = data.get_shape().as_list()\n",
    "        data   = tf.reshape(data, [-1, shape_flatten])\n",
    "        data   = tf.matmul(data, weights)\n",
    "        logits = tf.nn.bias_add(data, bias)\n",
    "        return logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, image_shape, helper_functions):\n",
    "        self.image_width    = image_shape[0]\n",
    "        self.image_height   = image_shape[1]\n",
    "        self.image_channels = image_shape[2]\n",
    "        self.no_filters     = [64, 128, 256, 512]\n",
    "        self.conv_strides   = [1, 2, 2, 1]\n",
    "        self.helper         = helper_functions\n",
    "    \n",
    "    \n",
    "    def d_propagate_forward(self, data):\n",
    "        \n",
    "        # data_shape = [-1, 28, 28, 1]\n",
    "        data = tf.reshape(data, [-1, self.image_width, self.image_height, self.image_channels])\n",
    "        \n",
    "        # disc_conv_1 shape = [-1, 28, 28, 64]\n",
    "        self.dwts_1   = self.helper.get_weights([3, 3, self.image_channels, self.no_filters[0]], 'd_w_1')\n",
    "        self.dbias_1  = self.helper.get_bias([self.no_filters[0]], 'd_b_1')\n",
    "        disc_conv_1  = self.helper.conv_block(data, self.dwts_1, self.dbias_1, 'd_conv_1', [1, 1, 1, 1], True, True)\n",
    "        \n",
    "        \n",
    "        # disc_conv_2 shape = [-1, 14, 14, 64]\n",
    "        self.dwts_2   = self.helper.get_weights([3, 3, self.no_filters[0], self.no_filters[0]], 'd_w_2')\n",
    "        self.dbias_2  = self.helper.get_bias([self.no_filters[0]], 'd_b_2')\n",
    "        disc_conv_2  = self.helper.conv_block(disc_conv_1, self.dwts_2, self.dbias_2, 'd_conv_2', self.conv_strides, False,\n",
    "                                                                 True)\n",
    "\n",
    "        \n",
    "        # disc_conv_3 shape = [-1, 7, 7, 128]\n",
    "        self.dwts_3   = self.helper.get_weights([3, 3, self.no_filters[0], self.no_filters[1]], 'd_w_3')\n",
    "        self.dbias_3  = self.helper.get_bias([self.no_filters[1]], 'd_b_3')\n",
    "        disc_conv_3  = self.helper.conv_block(disc_conv_2, self.dwts_3, self.dbias_3, 'd_conv_3', self.conv_strides, False, \n",
    "                                                                 True)\n",
    "        \n",
    "        \n",
    "        # disc_conv_4 shape = [-1, 4, 4, 256]\n",
    "        self.dwts_4   = self.helper.get_weights([3, 3, self.no_filters[1], self.no_filters[2]], 'd_w_4')\n",
    "        self.dbias_4  = self.helper.get_bias([self.no_filters[2]], 'd_b_4')\n",
    "        disc_conv_4  = self.helper.conv_block(disc_conv_3, self.dwts_4, self.dbias_4, 'd_conv_4', self.conv_strides, False, \n",
    "                                                                 True)\n",
    "        \n",
    "        \n",
    "        # disc_conv_5 shape = [-1, 2, 2, 512]\n",
    "        self.dwts_5   = self.helper.get_weights([3, 3, self.no_filters[2], self.no_filters[3]], 'd_w_5')\n",
    "        self.dbias_5  = self.helper.get_bias([self.no_filters[3]], 'd_b_5')\n",
    "        disc_conv_5  = self.helper.conv_block(disc_conv_4, self.dwts_5, self.dbias_5, 'd_conv_5', self.conv_strides, False,\n",
    "                                                                 True)\n",
    "        \n",
    "        \n",
    "        self.conv_5_shape = disc_conv_5.get_shape().as_list() # [-1, 4, 4, 512]\n",
    "        conv_5_shape_flat = self.helper.get_flatten_data_shape(disc_conv_5.get_shape().as_list())\n",
    "        self.shape_flatten = conv_5_shape_flat\n",
    "        self.dfc_wts  = self.helper.get_weights([conv_5_shape_flat, 1], 'd_fc_wt')\n",
    "        self.dfc_bias = self.helper.get_bias([1], 'd_fc_bias')\n",
    "        d_logits     = self.helper.fully_connected_layer(disc_conv_5, conv_5_shape_flat, self.dfc_wts, self.dfc_bias, 'd_fc_layer')\n",
    "        \n",
    "        return d_logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator:\n",
    "    def __init__(self, image_shape, noise_size, helper_functions, discriminator):\n",
    "        self.image_width = image_shape[0]\n",
    "        self.image_height = image_shape[1]\n",
    "        self.image_channels = image_shape[2]\n",
    "        self.noise_size = noise_size\n",
    "        self.no_gfilters = [512, 256, 128, 64, 3]\n",
    "        self.ghelper = helper_functions\n",
    "        self.disc_obj = discriminator\n",
    "        self.conv_strides = [1, 1, 1, 1]\n",
    "        \n",
    "        \n",
    "    def g_propagate_forward(self, data):\n",
    "        '''\n",
    "        gen_up_1   = [-1, 4, 4, 512]\n",
    "        gen_conv_2 = [-1, 4, 4, 256]\n",
    "        gen_up_2   = [-1, 8, 8, 256]\n",
    "        gen_conv_3 = [-1, 8, 8, 128]\n",
    "        gen_up_3   = [-1, 1, 32, 128]\n",
    "        gen_conv_4 = [-1, 32, 32, 64]\n",
    "        gen_up_5   = [-1, 64, 64, 64]\n",
    "        gen_conv_5 = [-1, 64, 64, 3]\n",
    "        '''\n",
    "        \n",
    "        width = self.disc_obj.conv_5_shape[1]\n",
    "        height = self.disc_obj.conv_5_shape[2]\n",
    "        channels = self.disc_obj.conv_5_shape[3]\n",
    "        \n",
    "        self.gwts_1 = self.ghelper.get_weights([100, self.disc_obj.shape_flatten], 'g_w_1')\n",
    "#         print('gwts ' + str(np.shape(self.gwts_1)))\n",
    "        data = tf.matmul(data, self.gwts_1)\n",
    "        data = tf.nn.relu(data)\n",
    "#         print('conv_5 ' + str(self.disc_obj.conv_5_shape))\n",
    "#         print('data ' + data.get_shape().as_list())\n",
    "        data = tf.reshape(data, [-1, width, height, channels]) \n",
    "        gen_up_1 = UpSampling2D()(data) \n",
    "        \n",
    "        self.gwts_2 = self.ghelper.get_weights([5, 5, 512, self.no_gfilters[1]], 'g_w_2')\n",
    "        gen_conv_2 = self.ghelper.conv_block(gen_up_1, self.gwts_2, None, 'g_conv_2', self.conv_strides, False, False)\n",
    "        gen_up_2 = UpSampling2D()(gen_conv_2) \n",
    "        \n",
    "        self.gwts_3 = self.ghelper.get_weights([5, 5, self.no_gfilters[1], self.no_gfilters[2]], 'g_w_3')\n",
    "        gen_conv_3 = self.ghelper.conv_block(gen_up_2, self.gwts_3, None, 'g_conv_3', self.conv_strides, False, False)\n",
    "        gen_up_3 = UpSampling2D()(gen_conv_3) \n",
    "        \n",
    "        self.gwts_4 = self.ghelper.get_weights([5, 5, self.no_gfilters[2], self.no_gfilters[3]], 'g_w_4')\n",
    "        gen_conv_4 = self.ghelper.conv_block(gen_up_3, self.gwts_4, None, 'g_conv_4', self.conv_strides, False, False)\n",
    "        gen_up_4 = UpSampling2D()(gen_conv_4) \n",
    "        \n",
    "        self.gwts_5 = self.ghelper.get_weights([5, 5, self.no_gfilters[3], self.no_gfilters[4]], 'g_w_5')\n",
    "        gen_conv_5 = tf.nn.conv2d(gen_up_4, self.gwts_5, [1, 1, 1, 1], padding = 'SAME')\n",
    "        return tf.nn.tanh(gen_conv_5)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DCGAN:\n",
    "    def __init__(self, image_shape, disc_lr, gen_lr, noise_size, batch_size, no_of_epochs,\n",
    "                                        train_data_path):\n",
    "        self.image_shape = image_shape\n",
    "        self.noise_size = noise_size\n",
    "        self.batch_size = batch_size\n",
    "        self.no_of_epochs = no_of_epochs\n",
    "        self.disc_lr = disc_lr\n",
    "        self.gen_lr = gen_lr\n",
    "        self.hf = helper_functions()\n",
    "        self.Discriminator = Discriminator(self.image_shape, self.hf)\n",
    "        self.Generator = Generator(self.image_shape, self.noise_size, self.hf, self.Discriminator)\n",
    "#         self.is_training = tf.placeholder(tf.bool, shape = ()) \n",
    "        self.disc_X = tf.placeholder(tf.float32, [None, self.image_shape[0], self.image_shape[1], self.image_shape[2]])\n",
    "        self.gen_X = tf.placeholder(tf.float32, [None, self.noise_size])\n",
    "        self.train_data_path = train_data_path\n",
    "#         self.test_data_path  = test_data_path\n",
    "        self.train_image_names = os.listdir(self.train_data_path + '/')\n",
    "        \n",
    "    \n",
    "    def get_input_batches(self, images_batch_names):\n",
    "        images_arr = []\n",
    "        for image_name in images_batch_names:\n",
    "            \n",
    "            image = cv2.resize(cv2.imread(self.train_data_path + \"/\" + image_name), (64, 64))\n",
    "            image = image / 255.0\n",
    "            images_arr.append(image)\n",
    "        return np.asarray(images_arr)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def calculate_loss(self, logits, labels):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "    \n",
    "    \n",
    "    def get_disc_and_gen_loss(self):\n",
    "        \n",
    "        # disc_X -> MNIST data || gen_X -> noisy_images\n",
    "        disc_logits_MNIST = self.Discriminator.d_propagate_forward(self.disc_X)\n",
    "        self.gen_noisy_images = self.Generator.g_propagate_forward(self.gen_X)\n",
    "        \n",
    "        disc_logits_noise = self.Discriminator.d_propagate_forward(self.gen_noisy_images)\n",
    "        \n",
    "        \n",
    "        disc_noise_labels = tf.zeros_like(disc_logits_noise)\n",
    "        disc_MNIST_labels = tf.ones_like(disc_logits_MNIST)\n",
    "        gen_noise_labels  = tf.ones_like(disc_logits_noise)\n",
    "        \n",
    "        disc_noise_loss = self.calculate_loss(disc_logits_noise, disc_noise_labels)\n",
    "        disc_MNIST_loss = self.calculate_loss(disc_logits_MNIST, disc_MNIST_labels)\n",
    "        \n",
    "        self.gen_loss  = self.calculate_loss(disc_logits_noise, gen_noise_labels)\n",
    "        self.disc_loss = tf.add(disc_noise_loss, disc_MNIST_loss)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def generate_sample_images(self, epoch):\n",
    "        noisy_data = np.random.uniform(-1, 1, (self.batch_size, self.noise_size))\n",
    "        images = self.session.run(self.gen_noisy_images, feed_dict = {self.gen_X : noisy_data})\n",
    "        images = images * 0.5 + 0.5\n",
    "        # scale between 0, 1\n",
    "        fig, axs = plt.subplots(c, r)\n",
    "        cnt = 0\n",
    "        for i in range(c):\n",
    "            for j in range(r):\n",
    "                axs[i, j].imshow(imgs[cnt, :, :, 0], cmap=\"gray\")\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"samples/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def train_GAN(self):\n",
    "        with tf.variable_scope('optim', reuse = tf.AUTO_REUSE):\n",
    "            self.get_disc_and_gen_loss()    \n",
    "            trainable_vars = tf.trainable_variables()\n",
    "            discriminator_vars = [var for var in trainable_vars if 'd' in var.name]\n",
    "            generator_vars = [var for var in trainable_vars if 'g' in var.name]\n",
    "\n",
    "            self.train_discriminator = tf.train.AdamOptimizer(self.disc_lr,beta1 = 0.5).minimize(self.disc_loss, \n",
    "                                                                                var_list = discriminator_vars)\n",
    "            self.train_generator = tf.train.AdamOptimizer(self.gen_lr, beta1 = 0.5).minimize(self.gen_loss, \n",
    "                                                                                var_list = generator_vars)\n",
    "\n",
    "            self.session = tf.Session()\n",
    "    #         with tf.Session() as session:\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            total_no_of_samples = len(self.train_image_names)\n",
    "\n",
    "            for epoch in range(0, self.no_of_epochs):\n",
    "                random.shuffle(self.train_image_names)\n",
    "\n",
    "                for index in range(0, total_no_of_samples, self.batch_size):\n",
    "                    train_batch = get_input_batches(self.train_image_names[index : index + batch_size])\n",
    "                    disc_noisy_batch = np.random.uniform(-1, 1, (self.batch_size, self.noise_size))\n",
    "\n",
    "                    _, disc_loss_ = self.session.run([self.train_discriminator, self.disc_loss], \n",
    "                                                feed_dict = {self.disc_X : train_batch, \n",
    "                                                             self.gen_X : disc_noisy_batch\n",
    "                                                             })\n",
    "\n",
    "\n",
    "                    gen_noisy_batch = np.random.uniform(-1, 1, (self.batch_size, self.noise_size))\n",
    "                    _, gen_loss_ = self.session.run([self.train_generator, self.gen_loss], \n",
    "                                                                feed_dict = {self.gen_X : gen_noisy_batch\n",
    "                                                                             })\n",
    "\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    print(epoch, disc_loss_, gen_loss_)\n",
    "\n",
    "                if epoch % 500 == 0:\n",
    "                    self.generate_sample_images(epoch)\n",
    "                    print(epoch, disc_loss_, gen_loss_)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    image_shape = [64, 64, 3]\n",
    "    disc_lr = 0.002\n",
    "    gen_lr  = 0.002\n",
    "    noise_size = 100\n",
    "    batch_size = 50\n",
    "    no_of_epochs = 1\n",
    "#     is_training = True\n",
    "    train_data_path = '/Users/vijay/Downloads/Datasets/Simpsons'\n",
    "    DC_GAN = DCGAN(image_shape, disc_lr, gen_lr, noise_size, batch_size, no_of_epochs, train_data_path)\n",
    "    DC_GAN.train_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
